{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline language modeling and Code setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input the dataset and read it in\n",
    "with open('cleaned_dataset.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset (in characters):  6199345\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of dataset (in characters): \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people youd expect to be involved in anything strange or mysterious, because they just didnt hold with such nonsense.\n",
      "\n",
      "Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.\n",
      "\n",
      "The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didnt think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursleys sister, but they hadnt met for several years; \n"
     ]
    }
   ],
   "source": [
    "#The first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"&'()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_abcdefghijklmnopqrstuvwxyz{|}\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "#Listing all the possible unique characters that occur in our dataset\n",
    "characters = sorted(list(set(text)))\n",
    "vocab_size = len(characters)\n",
    "print(''.join(characters))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need some strategy to tokenize the input text. When we say tokenize we mean convert the raw text as a string into some sequence of integers according to some vocabulary of possible elements.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Here in our case we are going to be building a character level language model - So will be translating individual characters into integers.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "We will be implementing encoders and decoders, but rather a simple one (as that should be enough for our usecase).\n",
    "\n",
    "But there are may others (Encoding texts into integers and also decoding them) which use different schema and different vocabularies:\n",
    "\n",
    "- Google uses [sentencepiece](https://github.com/google/sentencepiece): This encoder implements sub-word units. What that means is that it neither considers the entire word nor a single character. And that is what is usually adopted in practice.\n",
    "\n",
    "- OpenAI uses [tiktoken](https://github.com/openai/tiktoken): This uses BPE i.e. Bi Pair Encoding tokenizer and this what GPT uses. Here the vocabulary size is very large, almost upto 50,000 tokens.\n",
    "\n",
    "So here we have tradeoffs:\n",
    "- You can have very long sequence integers with a small vocabulary.\n",
    "- You can have very large vocabulary with a small sequence of integers.\n",
    "\n",
    "Now, we will be sticking to a character level tokenizer only and we are using a simple encoder and decoder. And our vocabulary size is pretty small i.e. `86` characters (so our tradeoff will be that we will have a large sequence of integers when it is encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating mapping from characters to integers\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(characters) }\n",
    "itos = { i:ch for i,ch in enumerate(characters) }\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "#Example to see how the encoding and decoding is happening\n",
    "# print(encode(\"harry potter\"))\n",
    "# print(decode(encode(\"harry potter\")))\n",
    "\n",
    "# Output:\n",
    "# [64, 57, 74, 74, 81, 1, 72, 71, 76, 76, 61, 74]\n",
    "# harry potter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6199345]) <built-in method size of Tensor object at 0x0000020CA1C9A2A0>\n",
      "tensor([38,  1, 74, 11,  1, 57, 70, 60,  1, 38, 74, 75, 11,  1, 29, 77, 74, 75,\n",
      "        68, 61, 81,  9,  1, 71, 62,  1, 70, 77, 69, 58, 61, 74,  1, 62, 71, 77,\n",
      "        74,  9,  1, 41, 74, 65, 78, 61, 76,  1, 29, 74, 65, 78, 61,  9,  1, 79,\n",
      "        61, 74, 61,  1, 72, 74, 71, 77, 60,  1, 76, 71,  1, 75, 57, 81,  1, 76,\n",
      "        64, 57, 76,  1, 76, 64, 61, 81,  1, 79, 61, 74, 61,  1, 72, 61, 74, 62,\n",
      "        61, 59, 76, 68, 81,  1, 70, 71, 74, 69, 57, 68,  9,  1, 76, 64, 57, 70,\n",
      "        67,  1, 81, 71, 77,  1, 78, 61, 74, 81,  1, 69, 77, 59, 64, 11,  1, 45,\n",
      "        64, 61, 81,  1, 79, 61, 74, 61,  1, 76, 64, 61,  1, 68, 57, 75, 76,  1,\n",
      "        72, 61, 71, 72, 68, 61,  1, 81, 71, 77, 60,  1, 61, 80, 72, 61, 59, 76,\n",
      "         1, 76, 71,  1, 58, 61,  1, 65, 70, 78, 71, 68, 78, 61, 60,  1, 65, 70,\n",
      "         1, 57, 70, 81, 76, 64, 65, 70, 63,  1, 75, 76, 74, 57, 70, 63, 61,  1,\n",
      "        71, 74,  1, 69, 81, 75, 76, 61, 74, 65, 71, 77, 75,  9,  1, 58, 61, 59,\n",
      "        57, 77, 75, 61,  1, 76, 64, 61, 81,  1, 66, 77, 75, 76,  1, 60, 65, 60,\n",
      "        70, 76,  1, 64, 71, 68, 60,  1, 79, 65, 76, 64,  1, 75, 77, 59, 64,  1,\n",
      "        70, 71, 70, 75, 61, 70, 75, 61, 11,  0,  0, 38, 74, 11,  1, 29, 77, 74,\n",
      "        75, 68, 61, 81,  1, 79, 57, 75,  1, 76, 64, 61,  1, 60, 65, 74, 61, 59,\n",
      "        76, 71, 74,  1, 71, 62,  1, 57,  1, 62, 65, 74, 69,  1, 59, 57, 68, 68,\n",
      "        61, 60,  1, 32, 74, 77, 70, 70, 65, 70, 63, 75,  9,  1, 79, 64, 65, 59,\n",
      "        64,  1, 69, 57, 60, 61,  1, 60, 74, 65, 68, 68, 75, 11,  1, 33, 61,  1,\n",
      "        79, 57, 75,  1, 57,  1, 58, 65, 63,  9,  1, 58, 61, 61, 62, 81,  1, 69,\n",
      "        57, 70,  1, 79, 65, 76, 64,  1, 64, 57, 74, 60, 68, 81,  1, 57, 70, 81,\n",
      "         1, 70, 61, 59, 67,  9,  1, 57, 68, 76, 64, 71, 77, 63, 64,  1, 64, 61,\n",
      "         1, 60, 65, 60,  1, 64, 57, 78, 61,  1, 57,  1, 78, 61, 74, 81,  1, 68,\n",
      "        57, 74, 63, 61,  1, 69, 77, 75, 76, 57, 59, 64, 61, 11,  1, 38, 74, 75,\n",
      "        11,  1, 29, 77, 74, 75, 68, 61, 81,  1, 79, 57, 75,  1, 76, 64, 65, 70,\n",
      "         1, 57, 70, 60,  1, 58, 68, 71, 70, 60, 61,  1, 57, 70, 60,  1, 64, 57,\n",
      "        60,  1, 70, 61, 57, 74, 68, 81,  1, 76, 79, 65, 59, 61,  1, 76, 64, 61,\n",
      "         1, 77, 75, 77, 57, 68,  1, 57, 69, 71, 77, 70, 76,  1, 71, 62,  1, 70,\n",
      "        61, 59, 67,  9,  1, 79, 64, 65, 59, 64,  1, 59, 57, 69, 61,  1, 65, 70,\n",
      "         1, 78, 61, 74, 81,  1, 77, 75, 61, 62, 77, 68,  1, 57, 75,  1, 75, 64,\n",
      "        61,  1, 75, 72, 61, 70, 76,  1, 75, 71,  1, 69, 77, 59, 64,  1, 71, 62,\n",
      "         1, 64, 61, 74,  1, 76, 65, 69, 61,  1, 59, 74, 57, 70, 65, 70, 63,  1,\n",
      "        71, 78, 61, 74,  1, 63, 57, 74, 60, 61, 70,  1, 62, 61, 70, 59, 61, 75,\n",
      "         9,  1, 75, 72, 81, 65, 70, 63,  1, 71, 70,  1, 76, 64, 61,  1, 70, 61,\n",
      "        65, 63, 64, 58, 71, 74, 75, 11,  1, 45, 64, 61,  1, 29, 77, 74, 75, 68,\n",
      "        61, 81, 75,  1, 64, 57, 60,  1, 57,  1, 75, 69, 57, 68, 68,  1, 75, 71,\n",
      "        70,  1, 59, 57, 68, 68, 61, 60,  1, 29, 77, 60, 68, 61, 81,  1, 57, 70,\n",
      "        60,  1, 65, 70,  1, 76, 64, 61, 65, 74,  1, 71, 72, 65, 70, 65, 71, 70,\n",
      "         1, 76, 64, 61, 74, 61,  1, 79, 57, 75,  1, 70, 71,  1, 62, 65, 70, 61,\n",
      "        74,  1, 58, 71, 81,  1, 57, 70, 81, 79, 64, 61, 74, 61, 11,  0,  0, 45,\n",
      "        64, 61,  1, 29, 77, 74, 75, 68, 61, 81, 75,  1, 64, 57, 60,  1, 61, 78,\n",
      "        61, 74, 81, 76, 64, 65, 70, 63,  1, 76, 64, 61, 81,  1, 79, 57, 70, 76,\n",
      "        61, 60,  9,  1, 58, 77, 76,  1, 76, 64, 61, 81,  1, 57, 68, 75, 71,  1,\n",
      "        64, 57, 60,  1, 57,  1, 75, 61, 59, 74, 61, 76,  9,  1, 57, 70, 60,  1,\n",
      "        76, 64, 61, 65, 74,  1, 63, 74, 61, 57, 76, 61, 75, 76,  1, 62, 61, 57,\n",
      "        74,  1, 79, 57, 75,  1, 76, 64, 57, 76,  1, 75, 71, 69, 61, 58, 71, 60,\n",
      "        81,  1, 79, 71, 77, 68, 60,  1, 60, 65, 75, 59, 71, 78, 61, 74,  1, 65,\n",
      "        76, 11,  1, 45, 64, 61, 81,  1, 60, 65, 60, 70, 76,  1, 76, 64, 65, 70,\n",
      "        67,  1, 76, 64, 61, 81,  1, 59, 71, 77, 68, 60,  1, 58, 61, 57, 74,  1,\n",
      "        65, 76,  1, 65, 62,  1, 57, 70, 81, 71, 70, 61,  1, 62, 71, 77, 70, 60,\n",
      "         1, 71, 77, 76,  1, 57, 58, 71, 77, 76,  1, 76, 64, 61,  1, 41, 71, 76,\n",
      "        76, 61, 74, 75, 11,  1, 38, 74, 75, 11,  1, 41, 71, 76, 76, 61, 74,  1,\n",
      "        79, 57, 75,  1, 38, 74, 75, 11,  1, 29, 77, 74, 75, 68, 61, 81, 75,  1,\n",
      "        75, 65, 75, 76, 61, 74,  9,  1, 58, 77, 76,  1, 76, 64, 61, 81,  1, 64,\n",
      "        57, 60, 70, 76,  1, 69, 61, 76,  1, 62, 71, 74,  1, 75, 61, 78, 61, 74,\n",
      "        57, 68,  1, 81, 61, 57, 74, 75, 24,  1])\n"
     ]
    }
   ],
   "source": [
    "# Now we will be encoding our entire dataset\n",
    "\n",
    "import torch #I used `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121`my CUDA version is 12.6\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape , data.size)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to the interesting part (atleast for me lol), we will be splitting the train and validation set. In our case we will be taking 90% for training and remaining for validation. The reason is we dont want our model to completely memorise the dataset and instead generate 'Harry Potter' like texts, hence we are witholding some information and will be using it to check for overfitting at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so now, we never feed our entire data into the model, as that would be computationally expensive and prohibitive. So we divide them into blocks and then group all those blocks into batches and then train them. Each batch is independently trainied and are not communicating with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[79, 57, 68, 67, 65, 70, 63,  1],\n",
      "        [64,  1, 57, 70, 63, 74, 81,  1],\n",
      "        [ 1, 69, 65, 60, 57, 65, 74,  9],\n",
      "        [ 1, 60, 65, 60,  1, 65, 76,  1]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[57, 68, 67, 65, 70, 63,  1, 57],\n",
      "        [ 1, 57, 70, 63, 74, 81,  1, 57],\n",
      "        [69, 65, 60, 57, 65, 74,  9,  1],\n",
      "        [60, 65, 60,  1, 65, 76,  1, 65]])\n",
      "----\n",
      "when input is [79] the target: 57\n",
      "when input is [79, 57] the target: 68\n",
      "when input is [79, 57, 68] the target: 67\n",
      "when input is [79, 57, 68, 67] the target: 65\n",
      "when input is [79, 57, 68, 67, 65] the target: 70\n",
      "when input is [79, 57, 68, 67, 65, 70] the target: 63\n",
      "when input is [79, 57, 68, 67, 65, 70, 63] the target: 1\n",
      "when input is [79, 57, 68, 67, 65, 70, 63, 1] the target: 57\n",
      "when input is [64] the target: 1\n",
      "when input is [64, 1] the target: 57\n",
      "when input is [64, 1, 57] the target: 70\n",
      "when input is [64, 1, 57, 70] the target: 63\n",
      "when input is [64, 1, 57, 70, 63] the target: 74\n",
      "when input is [64, 1, 57, 70, 63, 74] the target: 81\n",
      "when input is [64, 1, 57, 70, 63, 74, 81] the target: 1\n",
      "when input is [64, 1, 57, 70, 63, 74, 81, 1] the target: 57\n",
      "when input is [1] the target: 69\n",
      "when input is [1, 69] the target: 65\n",
      "when input is [1, 69, 65] the target: 60\n",
      "when input is [1, 69, 65, 60] the target: 57\n",
      "when input is [1, 69, 65, 60, 57] the target: 65\n",
      "when input is [1, 69, 65, 60, 57, 65] the target: 74\n",
      "when input is [1, 69, 65, 60, 57, 65, 74] the target: 9\n",
      "when input is [1, 69, 65, 60, 57, 65, 74, 9] the target: 1\n",
      "when input is [1] the target: 60\n",
      "when input is [1, 60] the target: 65\n",
      "when input is [1, 60, 65] the target: 60\n",
      "when input is [1, 60, 65, 60] the target: 1\n",
      "when input is [1, 60, 65, 60, 1] the target: 65\n",
      "when input is [1, 60, 65, 60, 1, 65] the target: 76\n",
      "when input is [1, 60, 65, 60, 1, 65, 76] the target: 1\n",
      "when input is [1, 60, 65, 60, 1, 65, 76, 1] the target: 65\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3007) # My dataset is different from what sensei is using, so i am using my own random number here :)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data #if the function call is for train then it considers train data else the val data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) #this one takes the random chunk of values\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) #x is the first array which will take the values\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #y is the second array which will consider the respective target values (\"the next character that needs to be predicted\")\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explaination for above is rather simple, in the first array we have the batch of data which we have considered and each row is the block of data.\n",
    "The second array shows us what the target value will be for the corresponding value in the first array. \n",
    "\n",
    "For example,\n",
    "In first array value is 79 -> so in target array its value will be 57\n",
    "In first array value is 79, 57 -> so in target array its value will be 68\n",
    "and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
